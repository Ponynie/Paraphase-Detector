{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create the results directory\n",
    "results_dir = 'results/train_test_results_sbert'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentence BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "def get_sentence_vector(sentence):\n",
    "    # Encode the single sentence and return the first (and only) embedding\n",
    "    return model.encode([sentence])[0]\n",
    "\n",
    "def create_combined_vector(s1, s2, method='concatenation'):\n",
    "    vec1 = get_sentence_vector(s1)\n",
    "    vec2 = get_sentence_vector(s2)\n",
    "    \n",
    "    if method == 'concatenation':\n",
    "        return np.concatenate((vec1, vec2))\n",
    "    elif method == 'mean':\n",
    "        return (vec1 + vec2) / 2\n",
    "    elif method == 'max_pooling':\n",
    "        return np.maximum(vec1, vec2)\n",
    "    else:\n",
    "        raise ValueError(\"Method not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filepath):\n",
    "    clean_data = {'Quality': [], '#1 ID': [], '#2 ID': [], '#1 String': [], '#2 String': []}\n",
    "    with open(filepath, 'r') as file:\n",
    "        next(file)  # Skip the header line\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 5:\n",
    "                clean_data['Quality'].append(parts[0])\n",
    "                clean_data['#1 ID'].append(parts[1])\n",
    "                clean_data['#2 ID'].append(parts[2])\n",
    "                clean_data['#1 String'].append(parts[3])\n",
    "                clean_data['#2 String'].append(parts[4])\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "    return pd.DataFrame(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess_data('JAIST-intern-data/MRPC_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features and Labels for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features and labels for training...\")\n",
    "X_train_concat = np.array([create_combined_vector(row['#1 String'], row['#2 String'], method='concatenation') for _, row in train_data.iterrows()])\n",
    "X_train_mean = np.array([create_combined_vector(row['#1 String'], row['#2 String'], method='mean') for _, row in train_data.iterrows()])\n",
    "X_train_max = np.array([create_combined_vector(row['#1 String'], row['#2 String'], method='max_pooling') for _, row in train_data.iterrows()])\n",
    "y_train = train_data['Quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM Models with Different Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "svm_models = {}\n",
    "\n",
    "print(\"Training SVM models...\")\n",
    "for kernel in kernels:\n",
    "    svm_concat = SVC(kernel=kernel)\n",
    "    svm_concat.fit(X_train_concat, y_train)\n",
    "    svm_models[f'concat_{kernel}'] = svm_concat\n",
    "    joblib.dump(svm_concat, os.path.join(results_dir, f'svm_concat_{kernel}.joblib'))  # Save the model\n",
    "\n",
    "    svm_mean = SVC(kernel=kernel)\n",
    "    svm_mean.fit(X_train_mean, y_train)\n",
    "    svm_models[f'mean_{kernel}'] = svm_mean\n",
    "    joblib.dump(svm_mean, os.path.join(results_dir, f'svm_mean_{kernel}.joblib'))  # Save the model\n",
    "\n",
    "    svm_max = SVC(kernel=kernel)\n",
    "    svm_max.fit(X_train_max, y_train)\n",
    "    svm_models[f'max_{kernel}'] = svm_max\n",
    "    joblib.dump(svm_max, os.path.join(results_dir, f'svm_max_{kernel}.joblib'))  # Save the model\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = preprocess_data('JAIST-intern-data/MRPC_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Test Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features and labels for testing...\")\n",
    "X_test_concat = np.array([create_combined_vector(row['#1 String'], row['#2 String'], method='concatenation') for _, row in test_data.iterrows()])\n",
    "X_test_mean = np.array([create_combined_vector(row['#1 String'], row['#2 String'], method='mean') for _, row in test_data.iterrows()])\n",
    "X_test_max = np.array([create_combined_vector(row['#1 String'], row['#2 String'], method='max_pooling') for _, row in test_data.iterrows()])\n",
    "y_test = test_data['Quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models and Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating models...\")\n",
    "results = []\n",
    "\n",
    "for method in ['concat', 'mean', 'max']:\n",
    "    for kernel in kernels:\n",
    "        model = svm_models[f'{method}_{kernel}']\n",
    "        if method == 'concat':\n",
    "            X_test = X_test_concat\n",
    "        elif method == 'mean':\n",
    "            X_test = X_test_mean\n",
    "        elif method == 'max':\n",
    "            X_test = X_test_max\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results.append({'Method': method, 'Kernel': kernel, 'Accuracy': accuracy})\n",
    "print(\"Evaluation complete.\")\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(results_dir, 'svm_evaluation_results.csv'), index=False)\n",
    "\n",
    "# Print results\n",
    "for index, row in results_df.iterrows():\n",
    "    print(f\"Method: {row['Method']}, Kernel: {row['Kernel']}, Accuracy: {row['Accuracy']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
